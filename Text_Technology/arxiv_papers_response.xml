<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%28cat%3Acs.AI%20OR%20cat%3Acs.CL%20OR%20cat%3Acs.HC%20OR%20cat%3Acs.LG%20OR%20cat%3Acs.MA%29%20AND%20agents%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=(cat:cs.AI OR cat:cs.CL OR cat:cs.HC OR cat:cs.LG OR cat:cs.MA) AND agents&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/7rjvGZ8JGgg3UC+JHyA6cL4iWFE</id>
  <updated>2025-07-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">28640</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.11882v5</id>
    <updated>2025-05-28T12:14:14Z</updated>
    <published>2025-02-17T15:09:45Z</published>
    <title>Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration</title>
    <summary>  Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. DPT-Agent can effectively
help LLMs convert correct slow thinking and reasoning into executable actions,
thereby improving performance. To the best of our knowledge, DPT-Agent is the
first language agent framework that achieves successful real-time simultaneous
human-AI collaboration autonomously. Code of DPT-Agent can be found in
https://github.com/sjtu-marl/DPT-Agent.
</summary>
    <author>
      <name>Shao Zhang</name>
    </author>
    <author>
      <name>Xihuai Wang</name>
    </author>
    <author>
      <name>Wenhao Zhang</name>
    </author>
    <author>
      <name>Chaoran Li</name>
    </author>
    <author>
      <name>Junru Song</name>
    </author>
    <author>
      <name>Tingyu Li</name>
    </author>
    <author>
      <name>Lin Qiu</name>
    </author>
    <author>
      <name>Xuezhi Cao</name>
    </author>
    <author>
      <name>Xunliang Cai</name>
    </author>
    <author>
      <name>Wen Yao</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <author>
      <name>Xinbing Wang</name>
    </author>
    <author>
      <name>Ying Wen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2025 Main. Camera Ready Version</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.11882v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.11882v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.00079v1</id>
    <updated>2024-09-30T16:52:51Z</updated>
    <published>2024-09-30T16:52:51Z</published>
    <title>Interactive Speculative Planning: Enhance Agent Efficiency through
  Co-design of System and User Interface</title>
    <summary>  Agents, as user-centric tools, are increasingly deployed for human task
delegation, assisting with a broad spectrum of requests by generating thoughts,
engaging with user proxies, and producing action plans. However, agents based
on large language models (LLMs) often face substantial planning latency due to
two primary factors: the efficiency limitations of the underlying LLMs due to
their large size and high demand, and the structural complexity of the agents
due to the extensive generation of intermediate thoughts to produce the final
output. Given that inefficiency in service provision can undermine the value of
automation for users, this paper presents a human-centered efficient agent
planning method -- Interactive Speculative Planning -- aiming at enhancing the
efficiency of agent planning through both system design and human-AI
interaction. Our approach advocates for the co-design of the agent system and
user interface, underscoring the importance of an agent system that can fluidly
manage user interactions and interruptions. By integrating human interruptions
as a fundamental component of the system, we not only make it more user-centric
but also expedite the entire process by leveraging human-in-the-loop
interactions to provide accurate intermediate steps. Code and data will be
released.
</summary>
    <author>
      <name>Wenyue Hua</name>
    </author>
    <author>
      <name>Mengting Wan</name>
    </author>
    <author>
      <name>Shashank Vadrevu</name>
    </author>
    <author>
      <name>Ryan Nadel</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <author>
      <name>Chi Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.00079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.00079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10974v2</id>
    <updated>2025-07-06T16:05:42Z</updated>
    <published>2025-06-12T17:59:32Z</published>
    <title>AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</title>
    <summary>  Large Language Model (LLM) agents have shown great potential in addressing
real-world data science problems. LLM-driven data science agents promise to
automate the entire machine learning pipeline, yet their real-world
effectiveness remains limited. Existing frameworks depend on rigid, pre-defined
workflows and inflexible coding strategies; consequently, they excel only on
relatively simple, classical problems and fail to capture the empirical
expertise that human practitioners bring to complex, innovative tasks. In this
work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework
that overcomes these deficiencies through three key advances: (1) a curated
expert knowledge base that grounds the agent in domain expert knowledge, (2) an
agentic knowledgeable tree search algorithm that strategically explores
possible solutions, and (3) a self-adaptive coding strategy that dynamically
tailors code generation to task complexity. Evaluations on two automated data
science benchmarks demonstrate that AutoMind delivers superior performance
versus state-of-the-art baselines. Additional analyses confirm favorable
effectiveness, efficiency, and qualitative solution quality, highlighting
AutoMind as an efficient and robust step toward fully automated data science.
</summary>
    <author>
      <name>Yixin Ou</name>
    </author>
    <author>
      <name>Yujie Luo</name>
    </author>
    <author>
      <name>Jingsheng Zheng</name>
    </author>
    <author>
      <name>Lanning Wei</name>
    </author>
    <author>
      <name>Shuofei Qiao</name>
    </author>
    <author>
      <name>Jintian Zhang</name>
    </author>
    <author>
      <name>Da Zheng</name>
    </author>
    <author>
      <name>Huajun Chen</name>
    </author>
    <author>
      <name>Ningyu Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ongoing work. Code is at https://github.com/innovatingAI/AutoMind</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.10974v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.10974v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.05268v4</id>
    <updated>2024-05-26T15:31:24Z</updated>
    <published>2024-01-10T16:57:24Z</published>
    <title>AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning</title>
    <summary>  Language agents have achieved considerable performance on various complex
question-answering tasks by planning with external tools. Despite the incessant
exploration in this field, existing language agent systems still struggle with
costly, non-reproducible data reliance and face the challenge of compelling a
single model for multiple functions. To this end, we introduce AutoAct, an
automatic agent learning framework for QA that does not rely on large-scale
annotated data and synthetic planning trajectories from closed-source models
(e.g., GPT-4). Given limited data with a tool library, AutoAct first
automatically synthesizes planning trajectories without any assistance from
humans or strong closed-source models. Then, AutoAct leverages a
division-of-labor strategy to automatically differentiate based on the target
task information and synthesized trajectories, producing a sub-agent group to
complete the task. We conduct comprehensive experiments with different LLMs,
which demonstrates that AutoAct yields better or parallel performance compared
to various strong baselines. Further analysis demonstrates the effectiveness of
the division-of-labor strategy, with the trajectory quality generated by
AutoAct generally outperforming that of others. Code will be available at
https://github.com/zjunlp/AutoAct.
</summary>
    <author>
      <name>Shuofei Qiao</name>
    </author>
    <author>
      <name>Ningyu Zhang</name>
    </author>
    <author>
      <name>Runnan Fang</name>
    </author>
    <author>
      <name>Yujie Luo</name>
    </author>
    <author>
      <name>Wangchunshu Zhou</name>
    </author>
    <author>
      <name>Yuchen Eleanor Jiang</name>
    </author>
    <author>
      <name>Chengfei Lv</name>
    </author>
    <author>
      <name>Huajun Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.05268v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.05268v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.03101v3</id>
    <updated>2025-02-21T05:04:27Z</updated>
    <published>2024-03-05T16:39:12Z</published>
    <title>KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents</title>
    <summary>  Large Language Models (LLMs) have demonstrated great potential in complex
reasoning tasks, yet they fall short when tackling more sophisticated
challenges, especially when interacting with environments through generating
executable actions. This inadequacy primarily stems from the lack of built-in
action knowledge in language agents, which fails to effectively guide the
planning trajectories during task solving and results in planning
hallucination. To address this issue, we introduce KnowAgent, a novel approach
designed to enhance the planning capabilities of LLMs by incorporating explicit
action knowledge. Specifically, KnowAgent employs an action knowledge base and
a knowledgeable self-learning strategy to constrain the action path during
planning, enabling more reasonable trajectory synthesis, and thereby enhancing
the planning performance of language agents. Experimental results on HotpotQA
and ALFWorld based on various backbone models demonstrate that KnowAgent can
achieve comparable or superior performance to existing baselines. Further
analysis indicates the effectiveness of KnowAgent in terms of planning
hallucinations mitigation. Code is available in
https://github.com/zjunlp/KnowAgent.
</summary>
    <author>
      <name>Yuqi Zhu</name>
    </author>
    <author>
      <name>Shuofei Qiao</name>
    </author>
    <author>
      <name>Yixin Ou</name>
    </author>
    <author>
      <name>Shumin Deng</name>
    </author>
    <author>
      <name>Shiwei Lyu</name>
    </author>
    <author>
      <name>Yue Shen</name>
    </author>
    <author>
      <name>Lei Liang</name>
    </author>
    <author>
      <name>Jinjie Gu</name>
    </author>
    <author>
      <name>Huajun Chen</name>
    </author>
    <author>
      <name>Ningyu Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL 2025 Findings. Project page:
  https://zjunlp.github.io/project/KnowAgent/ Code:
  https://github.com/zjunlp/KnowAgent</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.03101v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.03101v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
